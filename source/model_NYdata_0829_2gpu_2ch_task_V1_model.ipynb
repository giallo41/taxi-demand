{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unet Model - 2015 NY datasets Deep <br>\n",
    "\n",
    "### Date : 2015-01-01 ~ 2015-03-01 \n",
    "### training : 40 day (20% val ) / 20 day \n",
    "#### Summary : \n",
    "\n",
    "----------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name : MODEL_V1_NYC_0827_NY_001_2ch_test_base_1\n",
      "Output Folder : ../output_file/MODEL_V1_NYC/\n",
      "Tfgraph Folder : ../tfgraph/MODEL_V1_NYC/0827_NY_001_2ch_test_base_1\n",
      "Save Folder : ../saved_model/MODEL_V1_NYC/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import random as rn\n",
    "from utils import *\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "rn.seed(RANDOM_SEED)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "tf.set_random_seed(RANDOM_SEED)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, AveragePooling2D, UpSampling2D, Flatten, Reshape, Dropout, Conv2DTranspose\n",
    "from keras.layers import Concatenate, BatchNormalization, Add\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import InputLayer\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "\n",
    "\n",
    "###################\n",
    "## Hyperparam  - 맞게 수정하거나 Arg로 받아 온다. (Data Folder / Model Name / Model Ver ) 추가 \n",
    "###################\n",
    "\n",
    "\n",
    "LRATE = 1e-2\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 5\n",
    "SCALE = 'log' # or 'max' or 'log'\n",
    "NGPU = 2\n",
    "\n",
    "## Folder / File Name\n",
    "DATA_FOLDER = '/data/public/rw/prj-mobility/datasets/NY_data/2ch/'\n",
    "\n",
    "## Baseline Model Folder \n",
    "BASE_MODEL_NAME = 'MODEL_V1_NYC'\n",
    "MODEL_VER = '0827_NY_001_2ch_test_base_1'\n",
    "\n",
    "\n",
    "##################################################\n",
    "OUTPUT_FOLDER = '../output_file/'\n",
    "TF_FOLDER = '../tfgraph/'\n",
    "MODEL_SAVE_FOLDER = '../saved_model/'\n",
    "\n",
    "MODEL_NAME_ = BASE_MODEL_NAME+'_'+MODEL_VER\n",
    "OUTPUT_FOLDER = OUTPUT_FOLDER+BASE_MODEL_NAME+'/'\n",
    "TF_FOLDER = TF_FOLDER+BASE_MODEL_NAME+'/'+MODEL_VER\n",
    "MODEL_SAVE_FOLDER = MODEL_SAVE_FOLDER+BASE_MODEL_NAME+'/'\n",
    "\n",
    "print ('Model Name :',MODEL_NAME_)\n",
    "\n",
    "if os.path.isdir(OUTPUT_FOLDER) == False:\n",
    "    os.makedirs(OUTPUT_FOLDER)\n",
    "    print ('Output Folder :',OUTPUT_FOLDER, 'created')\n",
    "else :\n",
    "    print ('Output Folder :',OUTPUT_FOLDER)\n",
    "\n",
    "if os.path.isdir(TF_FOLDER) == False:\n",
    "    os.makedirs(TF_FOLDER)\n",
    "    print ('Tfgraph Folder :',TF_FOLDER, 'created')\n",
    "else :\n",
    "    print ('Tfgraph Folder :',TF_FOLDER)\n",
    "    \n",
    "if os.path.isdir(MODEL_SAVE_FOLDER) == False:\n",
    "    os.makedirs(MODEL_SAVE_FOLDER)\n",
    "    print ('Save Folder :',MODEL_SAVE_FOLDER, 'created')\n",
    "else :\n",
    "    print ('Save Folder :',MODEL_SAVE_FOLDER)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/public/rw/prj-mobility/10.195.12.143/kakaobrain/taxi/datasets/NY_data/train_st_x_2d.npz Loaded\n",
      "/data/public/rw/prj-mobility/10.195.12.143/kakaobrain/taxi/datasets/NY_data/train_end_x_2d.npz Loaded\n",
      "/data/public/rw/prj-mobility/10.195.12.143/kakaobrain/taxi/datasets/NY_data/train_st_y_2d.npz Loaded\n",
      "/data/public/rw/prj-mobility/10.195.12.143/kakaobrain/taxi/datasets/NY_data/temporal_train.npz Loaded\n",
      "/data/public/rw/prj-mobility/10.195.12.143/kakaobrain/taxi/datasets/NY_data/test_st_x_2d.npz Loaded\n",
      "/data/public/rw/prj-mobility/10.195.12.143/kakaobrain/taxi/datasets/NY_data/test_end_x_2d.npz Loaded\n",
      "/data/public/rw/prj-mobility/10.195.12.143/kakaobrain/taxi/datasets/NY_data/test_st_y_2d.npz Loaded\n",
      "/data/public/rw/prj-mobility/10.195.12.143/kakaobrain/taxi/datasets/NY_data/temporal_test.npz Loaded\n",
      "/data/public/rw/prj-mobility/10.195.12.143/kakaobrain/taxi/datasets/NY_data/train_coord_x_2d.npz Loaded\n",
      "/data/public/rw/prj-mobility/10.195.12.143/kakaobrain/taxi/datasets/NY_data/test_coord_x_2d.npz Loaded\n",
      "(1903, 10, 20, 10) (1903, 10, 20, 16) (1903, 10, 20, 1) (1903, 10, 20, 57) (1903, 10, 20, 57) (1903, 10, 20, 1)\n",
      "(476, 10, 20, 10) (476, 10, 20, 16) (476, 10, 20, 1) (476, 10, 20, 57) (476, 10, 20, 57) (476, 10, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## Data Load\n",
    "########################################\n",
    "\n",
    "\n",
    "NYC_FOLDER = '/data/public/rw/prj-mobility/10.195.12.143/kakaobrain/taxi/datasets/NY_data/'\n",
    "\n",
    "x_st_train = np.log(1.0+load_np_data(NYC_FOLDER+'train_st_x_2d.npz'))\n",
    "x_ed_train = np.log(1.0+load_np_data(NYC_FOLDER+'train_end_x_2d.npz'))\n",
    "y_train = np.log(1.0+load_np_data(NYC_FOLDER+'train_st_y_2d.npz'))\n",
    "temporal_train = load_np_data(NYC_FOLDER+'temporal_train.npz')\n",
    "\n",
    "\n",
    "x_st_test = np.log(1.0+load_np_data(NYC_FOLDER+'test_st_x_2d.npz'))\n",
    "x_ed_test = np.log(1.0+load_np_data(NYC_FOLDER+'test_end_x_2d.npz'))\n",
    "y_test = np.log(1.0+load_np_data(NYC_FOLDER+'test_st_y_2d.npz'))\n",
    "temporal_test = load_np_data(NYC_FOLDER+'temporal_test.npz')\n",
    "\n",
    "coord_train = load_np_data(NYC_FOLDER+'train_coord_x_2d.npz')/10 #args.coord\n",
    "coord_test = load_np_data(NYC_FOLDER+'test_coord_x_2d.npz')/10 #args.coord\n",
    "\n",
    "x_st_train = x_st_train[:-1]\n",
    "x_ed_train = x_ed_train[:-1]\n",
    "\n",
    "x_st_test = x_st_test[:-1]\n",
    "x_ed_test = x_ed_test[:-1]\n",
    "\n",
    "y_t2_train = y_train[1:]\n",
    "y_t2_test = y_test[1:]\n",
    "\n",
    "y_train = y_train[:-1]\n",
    "y_test = y_test[:-1]\n",
    "\n",
    "x_t2_temporal_train = temporal_train[1:]\n",
    "x_t2_temporal_test = temporal_test[1:]\n",
    "\n",
    "x_t1_temporal_train = temporal_train[:-1]\n",
    "x_t1_temporal_test = temporal_test[:-1]\n",
    "\n",
    "x_st_train = np.concatenate((x_st_train,coord_train[:-1]), axis=3)\n",
    "x_st_test = np.concatenate((x_st_test,coord_test[:-1]), axis=3)\n",
    "\n",
    "#y_train = np.concatenate( (y_train, y_t2_train), axis=3)\n",
    "#y_test = np.concatenate( (y_test, y_t2_test), axis=3)\n",
    "\n",
    "print (x_st_train.shape, x_ed_train.shape, y_train.shape, x_t1_temporal_train.shape,x_t2_temporal_train.shape, y_t2_train.shape)\n",
    "print (x_st_test.shape, x_ed_test.shape, y_test.shape, x_t1_temporal_test.shape,x_t2_temporal_test.shape, y_t2_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print (np.average(np.equal(x_st_train[1,:,:,-3], y_train[0,:,:,0])))\n",
    "print (np.average(np.equal(x_st_train[0,:,:,-3], x_st_train[1,:,:,-4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Before Split ----\n",
      "----- After Split ----\n",
      "(1524, 10, 20, 10) (1524, 10, 20, 16) (1524, 10, 20, 57) (1524, 10, 20, 57)\n",
      "(379, 10, 20, 10) (379, 10, 20, 16) (379, 10, 20, 57) (379, 10, 20, 57)\n"
     ]
    }
   ],
   "source": [
    "## Make Validation Data About 20% \n",
    "val_idx = 1524\n",
    "\n",
    "print ('----- Before Split ----')\n",
    "#print (x_st_train.shape, x_ed_train.shape, x_t1_temporal_train.shape, x_t0_temporal_train.shape, x_t2_temporal_train.shape)\n",
    "\n",
    "x_st_val = x_st_train[val_idx:]\n",
    "x_ed_val = x_ed_train[val_idx:]\n",
    "y_val = y_train[val_idx:]\n",
    "x_t1_temporal_val = x_t1_temporal_train[val_idx:]\n",
    "#x_t0_temporal_val = x_t0_temporal_train[val_idx:]\n",
    "x_t2_temporal_val = x_t2_temporal_train[val_idx:]\n",
    "\n",
    "\n",
    "x_st_train = x_st_train[:val_idx]\n",
    "x_ed_train = x_ed_train[:val_idx]\n",
    "y_train = y_train[:val_idx]\n",
    "x_t1_temporal_train = x_t1_temporal_train[:val_idx]\n",
    "#x_t0_temporal_train = x_t0_temporal_train[:val_idx]\n",
    "x_t2_temporal_train = x_t2_temporal_train[:val_idx]\n",
    "\n",
    "\n",
    "print ('----- After Split ----')\n",
    "print (x_st_train.shape, x_ed_train.shape, x_t1_temporal_train.shape, x_t2_temporal_train.shape)\n",
    "print (x_st_val.shape, x_ed_val.shape, x_t1_temporal_val.shape, x_t2_temporal_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session Reseted\n",
      "MODEL_V1_NYC_0827_NY_001_2ch_test_base_1 Model Created\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 10, 20, 57)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 10, 20, 10)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 10, 20, 16)   928         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 10, 20, 8)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 10, 20, 16)   272         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 10, 20, 24)   0           lambda_2[0][0]                   \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 10, 20, 64)   13888       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 10, 20, 64)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 10, 20, 64)   256         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 5, 10, 64)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 5, 10, 128)   73856       average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 5, 10, 128)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 5, 10, 128)   512         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 5, 10, 128)   147584      batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 5, 10, 128)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 5, 10, 128)   512         activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 5, 10, 256)   0           batch_normalization_2[0][0]      \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 5, 10, 128)   295040      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5, 10, 128)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 5, 10, 128)   512         activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 5, 10, 384)   0           batch_normalization_2[0][0]      \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 10, 20, 128)  196736      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 10, 20, 128)  0           conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 10, 20, 128)  512         activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 10, 20, 128)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 10, 20, 192)  0           dropout_1[0][0]                  \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 10, 20, 16)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 10, 20, 256)  442624      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 10, 20, 8)    1160        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 10, 20, 256)  0           conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 10, 20, 8)    32          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 10, 20, 256)  1024        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 10, 20, 8)    584         batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 10, 20, 256)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 10, 20, 8)    32          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 10, 20, 2)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 10, 20, 306)  0           dropout_2[0][0]                  \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "                                                                 lambda_2[0][0]                   \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 10, 20, 256)  78592       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 10, 20, 256)  0           conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 10, 20, 256)  1024        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 10, 20, 1)    257         batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 10, 20, 1)    0           conv2d_9[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,255,937\n",
      "Trainable params: 1,253,729\n",
      "Non-trainable params: 2,208\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "print ('Session Reseted')\n",
    "\n",
    "model_input_train_data = [x_st_train, x_ed_train, x_t1_temporal_train, x_t2_temporal_train]\n",
    "model_input_val_data=[x_st_val, x_ed_val, x_t1_temporal_val, x_t2_temporal_val]\n",
    "model_input_test_data = [x_st_test, x_ed_test, x_t1_temporal_test, x_t2_temporal_test]\n",
    "\n",
    "\n",
    "def make_temporal_model(st_data, ed_data, tmp_data):\n",
    "\n",
    "    im_h,im_w,im_c = st_data[0].shape\n",
    "\n",
    "    start_input = Input(shape=(st_data[0].shape))\n",
    "    end_input = Input(shape=(ed_data[0].shape))\n",
    "    t1_temporal_input = Input(shape=(tmp_data[0].shape))\n",
    "    t2_temporal_input = Input(shape=(tmp_data[0].shape))\n",
    "\n",
    "    input_tensors = [start_input, end_input, t1_temporal_input, t2_temporal_input]\n",
    "\n",
    "    coord_input = keras.layers.Lambda(lambda xin: xin[:,:,:,-2:] )(start_input)\n",
    "    start_input = keras.layers.Lambda(lambda xin: xin[:,:,:,:-2] )(start_input)\n",
    "\n",
    "    #Guassian Noise Augmentation for Training\n",
    "\n",
    "    ## 2a or 2b 추가 \n",
    "    #temp_conv2d_1st = Conv2D(16, kernel_size=(1,1), strides=(1,1))\n",
    "    #temp_conv2d_2nd = Conv2D(16, kernel_size=(1,1), strides=(1,1))\n",
    "\n",
    "    #net_t1_temp = temp_conv2d_1st(t1_temporal_input)\n",
    "    #net_t1_temp = temp_conv2d_2nd(net_t1_temp)\n",
    "    \n",
    "    #net_t2_temp = temp_conv2d_1st(t2_temporal_input)\n",
    "    #net_t2_temp = temp_conv2d_2nd(net_t2_temp)\n",
    "\n",
    "    # 1a or 2a 버전\n",
    "\n",
    "    net_t1_temp = Conv2D(16, kernel_size=(1,1), strides=(1,1))(t1_temporal_input)\n",
    "    net_t1_temp = Conv2D(16, kernel_size=(1,1), strides=(1,1))(net_t1_temp)\n",
    "\n",
    "    #net_t2_temp = Conv2D(16, kernel_size=(1,1), strides=(1,1))(t2_temporal_input)\n",
    "    #net_t2_temp = Conv2D(16, kernel_size=(1,1), strides=(1,1))(net_t2_temp)\n",
    "\n",
    "    net1 = layers.concatenate([start_input, net_t1_temp], axis=-1)\n",
    "    net1 = Conv2D(64, kernel_size=(3,3), activation=None, padding='same')(net1)\n",
    "    net1 = layers.Activation('relu')(net1)\n",
    "    net1 = BatchNormalization()(net1)\n",
    "\n",
    "    net11 = AveragePooling2D(pool_size=(2,2), strides=(2,2))(net1)\n",
    "\n",
    "    net2 = Conv2D(128, kernel_size=(3,3), activation=None, padding='same')(net11)\n",
    "    net2 = layers.Activation('relu')(net2)\n",
    "    net2 = BatchNormalization()(net2)\n",
    "\n",
    "    net3 = Conv2D(128, kernel_size=(3,3), activation=None, padding='same')(net2)\n",
    "    net3 = layers.Activation('relu')(net3)\n",
    "    net3 = BatchNormalization()(net3)\n",
    "    net33 = layers.concatenate([net2, net3], axis=-1)\n",
    "\n",
    "    net4 = Conv2D(128, kernel_size=(3,3), activation=None, padding='same')(net33)\n",
    "    net4 = layers.Activation('relu')(net4)\n",
    "    net4 = BatchNormalization()(net4)\n",
    "    net44 = layers.concatenate([net2, net3, net4], axis=-1)\n",
    "    #net4 = layers.Add()([net2, net4])\n",
    "\n",
    "    net5 = Conv2DTranspose(128, kernel_size=(2,2), strides=(2,2), padding='same')(net44)\n",
    "    net5 = layers.Activation('relu')(net5)\n",
    "    net5 = BatchNormalization()(net5)\n",
    "    net5 = Dropout(0.5)(net5)\n",
    "\n",
    "    net5 = layers.concatenate([net5, net1], axis=-1)\n",
    "\n",
    "    net6 = Conv2DTranspose(256, kernel_size=(3,3), strides=(1,1), padding='same')(net5)\n",
    "    net6 = layers.Activation('relu')(net6)\n",
    "    net6 = BatchNormalization()(net6)\n",
    "    net6 = Dropout(0.5)(net6)\n",
    "\n",
    "    net_end = Conv2D(8, kernel_size=(3,3), padding='same')(end_input)\n",
    "    net_end = BatchNormalization()(net_end)\n",
    "    net_end = Conv2D(8, kernel_size=(3,3), padding='same')(net_end)\n",
    "    net_end = BatchNormalization()(net_end)\n",
    "    \n",
    "    \n",
    "\n",
    "    net71 = layers.concatenate([net6, net_t1_temp, net_end, start_input, end_input, coord_input], axis=-1)\n",
    "    net71 = Conv2DTranspose(256, kernel_size=(1,1), padding='same')(net71)\n",
    "    net71 = layers.Activation('relu')(net71)\n",
    "    net71 = BatchNormalization()(net71)\n",
    "\n",
    "    #net72 = layers.concatenate([net6, start_input, net_end, end_input, net_t2_temp, coord_input], axis=-1)\n",
    "    #net72 = Conv2DTranspose(256, kernel_size=(1,1), padding='same')(net72)\n",
    "    #net72 = layers.Activation('relu')(net72)\n",
    "    #net72 = BatchNormalization()(net72)\n",
    "\n",
    "    t1_output = Conv2D(1, kernel_size=(1,1), padding='same')(net71)\n",
    "    output = layers.Activation('relu')(t1_output)\n",
    "\n",
    "    #t2_output = Conv2D(1, kernel_size=(1,1), padding='same')(net72)\n",
    "    #t2_output = layers.Activation('relu')(t2_output)\n",
    "\n",
    "    #output = layers.concatenate([t1_output, t2_output], axis=-1)\n",
    "\n",
    "    model = Model(inputs=input_tensors, outputs=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "b_model = make_temporal_model(x_st_train,x_ed_train, x_t1_temporal_train)\n",
    "model = multi_gpu_model(b_model, gpus=NGPU)\n",
    "print (MODEL_NAME_, 'Model Created')\n",
    "print (b_model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## Callback \n",
    "########################################\n",
    "\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class SGDLearningRateTracker(Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        optimizer = self.model.optimizer\n",
    "        lr = K.eval(optimizer.lr) # K.eval(optimizer.lr * (1. / (1. + optimizer.decay * optimizer.iterations)))\n",
    "        print('LR: {:.6f}'.format(lr))\n",
    "\n",
    "tb_hist = keras.callbacks.TensorBoard(log_dir=TF_FOLDER, histogram_freq=0, write_graph=True, write_images=True)\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_invlog_mape_tr10', min_delta=0, patience=10, verbose=0, mode='min')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \n",
    "\n",
    "#### MSE pretraining -> MAE fine-tuning \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1524 samples, validate on 379 samples\n",
      "Epoch 1/5\n",
      " - 7s - loss: 5.8302 - mean_absolute_error: 1.1699 - invlog_mape_tr10: 2331274806.2314 - val_loss: 119.3727 - val_mean_absolute_error: 6.5410 - val_invlog_mape_tr10: 23354522805743323709440.0000\n",
      "LR: 0.010000\n",
      "Epoch 2/5\n",
      " - 1s - loss: 0.5090 - mean_absolute_error: 0.4208 - invlog_mape_tr10: 0.7774 - val_loss: 34.8769 - val_mean_absolute_error: 3.3870 - val_invlog_mape_tr10: 893218672.0422\n",
      "LR: 0.010000\n",
      "Epoch 3/5\n",
      " - 1s - loss: 0.3147 - mean_absolute_error: 0.3216 - invlog_mape_tr10: 0.5877 - val_loss: 4.0851 - val_mean_absolute_error: 1.1867 - val_invlog_mape_tr10: 119.5614\n",
      "LR: 0.010000\n",
      "Epoch 4/5\n",
      " - 1s - loss: 0.2344 - mean_absolute_error: 0.2755 - invlog_mape_tr10: 0.4487 - val_loss: 1.1193 - val_mean_absolute_error: 0.6414 - val_invlog_mape_tr10: 6.5599\n",
      "LR: 0.010000\n",
      "Epoch 5/5\n",
      " - 1s - loss: 0.1907 - mean_absolute_error: 0.2476 - invlog_mape_tr10: 0.3945 - val_loss: 0.4303 - val_mean_absolute_error: 0.4175 - val_invlog_mape_tr10: 1.5863\n",
      "LR: 0.010000\n",
      "Model Saved ../saved_model/MODEL_V1_NYC/MODEL_V1_NYC_0827_NY_001_2ch_test_base_1st1.h5\n",
      "../output_file/MODEL_V1_NYC/MODEL_V1_NYC_0827_NY_001_2ch_test_base_1st1_loss_metric_invmape_0.418.csv  Val loss Saved\n",
      "Train on 1524 samples, validate on 379 samples\n",
      "Epoch 1/5\n",
      " - 4s - loss: 0.3058 - mean_absolute_error: 0.3058 - invlog_mape_tr10: 1.6812 - val_loss: 0.4303 - val_mean_absolute_error: 0.4303 - val_invlog_mape_tr10: 1.1794\n",
      "LR: 0.010000\n",
      "Epoch 2/5\n",
      " - 1s - loss: 0.2088 - mean_absolute_error: 0.2088 - invlog_mape_tr10: 0.3193 - val_loss: 0.2406 - val_mean_absolute_error: 0.2406 - val_invlog_mape_tr10: 0.4871\n",
      "LR: 0.010000\n",
      "Epoch 3/5\n",
      " - 1s - loss: 0.2011 - mean_absolute_error: 0.2011 - invlog_mape_tr10: 0.2985 - val_loss: 0.2404 - val_mean_absolute_error: 0.2404 - val_invlog_mape_tr10: 0.5259\n",
      "LR: 0.010000\n",
      "Epoch 4/5\n",
      " - 1s - loss: 0.1995 - mean_absolute_error: 0.1995 - invlog_mape_tr10: 0.3049 - val_loss: 0.2454 - val_mean_absolute_error: 0.2454 - val_invlog_mape_tr10: 0.5274\n",
      "LR: 0.010000\n",
      "Epoch 5/5\n",
      " - 1s - loss: 0.1895 - mean_absolute_error: 0.1895 - invlog_mape_tr10: 0.2609 - val_loss: 0.2279 - val_mean_absolute_error: 0.2279 - val_invlog_mape_tr10: 0.3961\n",
      "LR: 0.010000\n",
      "\n",
      "--- Train Time : 0.01 hour  ---\n",
      "--- # of Epochs: 10  ---\n",
      "Model Saved ../saved_model/MODEL_V1_NYC/MODEL_V1_NYC_0827_NY_001_2ch_test_base_1st2.h5\n",
      "../output_file/MODEL_V1_NYC/MODEL_V1_NYC_0827_NY_001_2ch_test_base_1st2_loss_metric_invmape_0.418.csv  Val loss Saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "start_time = time.time() \n",
    "\n",
    "\n",
    "model.compile(loss=['mean_squared_error'], optimizer=Adam(lr=LRATE,  decay=0.01), metrics=['mean_absolute_error', invlog_mape_tr10])\n",
    "history = model.fit(model_input_train_data , y_train,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  epochs=EPOCHS,\n",
    "                  callbacks=[tb_hist, SGDLearningRateTracker()],\n",
    "                  shuffle = True,\n",
    "                  verbose=2\n",
    "                  ,validation_data=(model_input_val_data, y_val)\n",
    "                 )\n",
    "    \n",
    "val_loss = history.history['val_loss']\n",
    "val_metric = history.history['val_mean_absolute_error']\n",
    "\n",
    "save_model_name = MODEL_SAVE_FOLDER+MODEL_NAME_+'st1.h5'\n",
    "b_model.save(save_model_name)\n",
    "print ('Model Saved', save_model_name)\n",
    "history_save(val_loss, val_metric, model_name =  MODEL_NAME_+'st1', output_folder=OUTPUT_FOLDER)\n",
    "\n",
    "\n",
    "model.compile(loss=['mean_absolute_error'], optimizer=Adam(lr=LRATE,  decay=0.01), metrics=['mean_absolute_error', invlog_mape_tr10])\n",
    "history = model.fit(model_input_train_data , y_train,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  epochs=EPOCHS,\n",
    "                  callbacks=[tb_hist, early_stopping, SGDLearningRateTracker()],\n",
    "                  shuffle = True,\n",
    "                  verbose=2\n",
    "                  ,validation_data=(model_input_val_data, y_val)\n",
    "                 )\n",
    "\n",
    "val_loss2 = history.history['val_loss']\n",
    "val_metric2 = history.history['val_mean_absolute_error']\n",
    "\n",
    "end_time = time.time()\n",
    "n_epochs = len(val_metric)+len(val_metric2)\n",
    "\n",
    "print ('')\n",
    "print(\"--- Train Time : %0.2f hour  ---\" %(  (end_time - start_time)/3600  ))\n",
    "print(\"--- # of Epochs: %0.f  ---\" %( n_epochs ) )\n",
    "\n",
    "\n",
    "save_model_name = MODEL_SAVE_FOLDER+MODEL_NAME_+'st2.h5'\n",
    "b_model.save(save_model_name)\n",
    "print ('Model Saved', save_model_name)\n",
    "history_save(val_loss, val_metric, model_name =  MODEL_NAME_+'st2', output_folder=OUTPUT_FOLDER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Output Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Test datasets Performance\n",
      "- MAPE(11 or more) : 0.401\n",
      "- RMSE(11 or more) : 102.671\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test = model.predict(model_input_test_data)\n",
    "\n",
    "print ('')\n",
    "print (\"## Test datasets Performance\")\n",
    "print (\"- MAPE(11 or more) : %.3f\"%mape_trs(inverse_logscale(y_test), inverse_logscale(pred_test), 11))\n",
    "print (\"- RMSE(11 or more) : %.3f\"%rmse_trs(inverse_logscale(y_test), inverse_logscale(pred_test), 11))\n",
    "print ('')\n",
    "\n",
    "#print (\"- T1 MAPE(11 or more) : %.3f\"%mape_trs(inverse_logscale(y_test[:,:,:,:1]), inverse_logscale(pred_test[:,:,:,:1]), 11))\n",
    "#print (\"- T2 MAPE(11 or more) : %.3f\"%mape_trs(inverse_logscale(y_test[:,:,:,-1:]), inverse_logscale(pred_test[:,:,:,-1:]), 11))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../output_file/MODEL_V1_NYC/MODEL_V1_NYC_0827_NY_001_2ch_test_base_1_loss_metric_invmape_0.418.csv  Val loss Saved\n",
      "Test True saved :  ../output_file/MODEL_V1_NYC/MODEL_V1_NYC_0827_NY_001_2ch_test_base_1_gt.csv\n",
      "Test Pred saved :  ../output_file/MODEL_V1_NYC/MODEL_V1_NYC_0827_NY_001_2ch_test_base_1_pred.csv\n",
      "\n",
      "## Model Name :  ../output_file/MODEL_V1_NYC/MODEL_V1_NYC_0827_NY_001_2ch_test_base_1\n",
      "\n",
      "## ---- Base Metric -----\n",
      "- True Max 1099 , Pred Max 1250\n",
      "- True Avg 40.84 , Pred Avg 55.61\n",
      "- MAPE(+1) : 0.271\n",
      "- RMSE : 52.141\n",
      "\n",
      "## ---- Trs MAPE Metric -----\n",
      "- 1 or More  MAPE : 0.629\n",
      "- 2 or More  MAPE : 0.533\n",
      "- 3 or More  MAPE : 0.490\n",
      "- 4 or More  MAPE : 0.439\n",
      "- 5 or More  MAPE : 0.439\n",
      "- 6 or More  MAPE : 0.418\n",
      "- 7 or More  MAPE : 0.409\n",
      "- 8 or More  MAPE : 0.409\n",
      "- 9 or More  MAPE : 0.406\n",
      "- 10 or More  MAPE : 0.403\n",
      "- 11 or More  MAPE : 0.401\n",
      "- 12 or More  MAPE : 0.401\n",
      "- 13 or More  MAPE : 0.398\n",
      "- 14 or More  MAPE : 0.398\n",
      "- 15 or More  MAPE : 0.396\n",
      "\n",
      "## ---- Trs MAA Metric -----\n",
      "- Range 0 %  MAA : 52.82 %\n",
      "- Range 10 %  MAA : 56.13 %\n",
      "- Range 20 %  MAA : 61.16 %\n",
      "- Range 30 %  MAA : 65.27 %\n",
      "- Range 40 %  MAA : 71.35 %\n",
      "- Range 50 %  MAA : 76.38 %\n",
      "- Range 60 %  MAA : 79.29 %\n",
      "- Range 70 %  MAA : 82.32 %\n",
      "- Range 80 %  MAA : 84.01 %\n",
      "- Range 90 %  MAA : 84.91 %\n",
      "- Range 100 %  MAA : 100.00 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history_save(val_loss, val_metric, model_name =  MODEL_NAME_, output_folder=OUTPUT_FOLDER)\n",
    "make_test_ouput(model, model_input_test_data , y_test, model_name=MODEL_NAME_, norm=SCALE, output_folder=OUTPUT_FOLDER)\n",
    "#make_test_2ch_ouput(model, model_input_test_data , y_test, model_name=MODEL_NAME_, norm=SCALE, output_folder=OUTPUT_FOLDER)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved MODEL_V1_NYC_0827_NY_001_2ch_test_base_1MODEL_V1_NYC_0827_NY_001_2ch_test_base_1.h5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "save_model_name = MODEL_NAME_+MODEL_NAME_+'.h5'\n",
    "b_model.save(save_model_name)\n",
    "print ('Model Saved', save_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
